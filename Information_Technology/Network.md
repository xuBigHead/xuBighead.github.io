# 目录

[TOC]



# 网络

## 网络层次

| 网络层次 | 网络协议                       |
| -------- | ------------------------------ |
| 应用层   | HTTP（90年代），gRPC（80年代） |
| 传输层   | TCP（70年代），UDP             |



### 物理层

数据链路层类似通过网线将两台电脑连接起来，规定好彼此的硬件接口。**这一层就是物理层，这些规定就是物理层协议** 。



### 数据链路层

通过交换机连接多台电脑，这样连接起来的网络，称为局域网，也可以称为以太网（以太网是局域网的一种）。

在局域网中，需要标识每台机器才能指定要和哪台机器通信，这个标识就是**硬件地址MAC**，硬件地址随机器的生产就被确定，永久性唯一。

交换机就会把消息发送到对应硬件地址的机器。

这种不管底层的网线接口如何发送，在物理层之上创建一个新的层次，这就是**数据链路层** 。



### 网络层

通过路由器来连接多个局域网，此时在网络规模较大的情况下就无法使用硬件地址来标识所有的机器，此时就会使用**IP地址**。

**局域网中，动态维护一个MAC地址与IP地址的映射关系，根据目的IP地址就可以寻找到机器的MAC地址进行发送** 。

这种不管底层如何去选择机器，只要知道IP地址，就可以和目标进行通信，就是**网络层**。网络层的核心作用就是**提供主机之间的逻辑通信** 。



### 运输层

一台机器有多个进程，进程之间进行不同的网络通信，此时就需要运输层来**提供进程之间的逻辑通信** 。

运输层通过socket（套接字），将网络信息进行进一步的拆分，不同的应用进程可以独立进行网络请求，互不干扰。这里的进程可以是主机之间，也可以是同个主机。



在运输层之下的网络层，是不知道该数据包属于哪个进程，他只负责数据包的接收与发送。运输层则负责接收不同进程的数据交给网络层，同时把网络层的数据拆分交给不同的进程。从上往下汇聚到网络层，称为**多路复用**，从下往上拆分，称为**多路拆分** 。

运输层表现受网络层限制，网络层是运输层的底层支持。所以运输层是无法决定自己带宽、时延等的上限。但可以基于网络层开发更多的特性：如可靠传输。网络层只负责尽力把数据包从一端发送到另一端，而不保证数据可以到达且完整。



#### Socket

最简单的运输层协议，就是提供进程之间的独立通信 ，但底层的实现，是**socket之间的独立通信** 。在网络层中，IP地址是一个主机逻辑地址，而在运输层中，socket是一个进程的逻辑地址；当然，一个进程可以拥有多个socket。应用进程可以通过监听socket，来获取这个socket接受到的消息。

socket并不是一个实实在在的东西，而是运输层抽象出来的一个对象。运输层增加了**端口**这个概念，来区分不同的socket。端口可以理解为一个主机上有很多的网络通信口，每个端口都有一个端口号，端口的数量由运输层协议确定。

不同的运输层协议对socket有不同的定义方式。在UDP协议中，使用目标IP+目标端口号来定义一个socket；在TCP中使用目标IP+目标端口号+源IP+源端口号来定义一个socket。只需要在运输层报文的头部附加上这些信息，目标主机就会知道要发送给哪个socket，对应监听该socket的进程就可获得信息。



#### 运输层协议

##### TCP和UDP

运输层的协议就是大名鼎鼎的TCP和UDP。其中，UDP是最精简的运输层协议，只实现了进程间的通信；而TCP在UDP的基础上，实现了可靠传输、流量控制、拥塞控制、面向连接等等特性，同时也更加复杂。

除此之外，还有更多更优秀的运输层协议，但目前广为使用的，就是TCP和UDP。



### 应用层

在计算机网络上开发出形形式式的应用：如web网页的http，文件传输ftp等等。这一层称为**应用层**。

应用层还可以进一步拆分出表示层、会话层，但他们的本质特点都没有改变：**完成具体的业务需求** 。



#### 会话层



#### 表示层



### 总结

#### 五级网络层次

![20220506-1](../../Image/2022/05/220506-1.png)



1. 最底层物理层，负责两个机器之间通过硬件的直接通信；
2. 数据链路层使用硬件地址在局域网中进行寻址，实现局域网通信；
3. 网络层通过抽象IP地址实现主机之间的逻辑通信；
4. 运输层在网络层的基础上，对数据进行拆分，实现应用进程的独立网络通信；
5. 应用层在运输层的基础上，根据具体的需求开发形形式式的功能。



分层并不是在物理上的分层，而是逻辑上的分层。通过对底层逻辑的封装，使得上层的开发可以直接依赖底层的功能而无需理会具体的实现，简便了开发。这种分层的思路，也就是责任链设计模式，通过层层封装，把不同的职责独立起来，更加方便开发、维护等等。



#### 七层网络层次

![20220506-2](../../Image/2022/05/220506-2.png)





## 网络协议

### TCP协议

TCP（Transmission Control Protocol，传输控制协议）是基于连接的协议，在正式收发数据前，必须和对方建立可靠的连接。

TCP是有三个特点，**面向连接**、**可靠**、基于**字节流**。



#### 报文结构

<img src="../../Image/2022/05/220506-3.png" alt="20220506-3" style="zoom: 67%;" />



TCP首部固定长度是20字节，下面还有4字节是可选的。

| 头部参数         | 字节数  | 作用                                                         |
| ---------------- | ------- | ------------------------------------------------------------ |
| 源端口和目的端口 | 各2字节 | socket通过端口号和IP来定义，这里表示发出消息的主机端口以及接收消息的主机端口。 |
| 序号             | 4字节   | TCP传输的数据流中每一个字节都编上一个序号，这里序号的指的是本报文发送的数据的第一个字节的序号。<br />序号取值范围是[0, 2^32 - 1]。 |
| 确认号           | 4字节   | 期望收到的下一个报文数据的第一个字节的序号。                 |
| 数据偏移         | 4位     | 即首部长度，指TCP报文的数据起始处和TCP报文的起始处距离。     |
| 保留             | 6位     | 保留为今后使用，目前设为0。                                  |
| 紧急URG          | 1位     | 为1时表示为报文含有紧急数据，应尽快传送。即表示优先级高的数据。 |
| 确认ACK          | 1位     | 收到报文向发送方发送确认报文时设置该标志位为1。              |
| 推送PSH          | 1位     | 收到PSH=1时，尽快交付接收应用进程，而不是等到整个缓存填满了后再向上交付。 |
| 复位RST          | 1位     | 为1时表明TCP连接中出现严重问题（如主机奔溃），必须放弃连接，然后再重新建立运输连接。 |
| 同步SYN          | 1位     | 为1时表明这是一个连接请求或连接接受报文。                    |
| 终止FIN          | 1位     | 用来释放一个连接，为1时表明此报文的发送端的数据已发送完毕，并要求释放运输连接 |
| 窗口字段         | 2字节   | 发送方接受缓存区剩下的**字节数**。                           |



选项字段中包含以下其他选项：

![20220506-5](../../Image/2022/05/220506-5.png)



tcp协议为每一个连接建立了发送缓冲区，从建立链接后的第一个字节的序列号为0，后面每个字节的序列号就会增加1，发送数据时，从数据缓冲区取一部分数据组成发送报文，在tcp协议头中会附带序列号和长度，接收端在收到数据后需要回复确认报文，确认报文中的ack等于接受序列号加长度，也就是下包数据发送的起始序列号，这样一问一答的发送方式，能够使发送端确认发送的数据已经被对方收到，发送端也可以发送一次的连续的多包数据，接受端只需要回复一次ack就可以了。



#### 传输形式

TCP并不是把应用层传输过来的数据直接加上首部然后发送给目标，而是把数据看成一个字节流，给数据标上序号之后分批发送。这就是TCP的**面向字节流** 特性。



![20220506-6](../../Image/2022/05/220506-6.png)



- TCP会以流的形式从应用层读取数据并存放在自己的发送缓存区中，同时为这些**字节**标上序号
- TCP会从发送方缓冲区选择适量的字节组成TCP报文，通过网络层发送给目标
- 目标会读取字节并存放在自己的接收方缓冲区中，并在合适的时候交付给应用层



面向字节流的好处是无需一次存储过大的数据占用太多内存，坏处是无法知道这些字节代表的意义，例如应用层发送一个音频文件和一个文本文件，对于TCP来说就是一串字节流，没有意义可言，这会导致粘包以及拆包问题。



##### 分块传输

TCP为了避免传输中断时要将整个数据重传，运输层传输数据时，会将数据包拆成多个报文分开发送。此时传输中断，只需重传该数据报文即可。

拆分的报文过小时，会使得报文的固定首部的占比太大。例如1000字节，每个报文首部是40字节，如果拆分成10个报文，那么只需要传输400字节的首部；而如果拆分成1000个，那么需要传输40000字节的首部，效率就极大地降低了。



##### 粘包与拆包

因为TCP传输的是**无边界**的数据流，所以TCP分块传输时会发生粘包和拆包。粘包是指将两个数据的部分内容拆分到了同一个报文；拆包是指目标进程应用层在接收到数据之后，需要把这些数据拆分成正确的两个数据。

粘包与拆包都是应用层需要解决的问题，可以在每个文件的最后附加上一些特殊的字节，如换行符；或者控制每个报文只包含一个文件的数据，不足的用0补充等等。



#### 可靠传输

对于应用层来说，他就是一个可靠传输的底层支持服务；而运输层底层采用了网络层的不可靠传输。虽然在网络层甚至数据链路层就可以使用协议来保证数据传输的可靠性，但这样网络的设计会更加复杂、效率会随之降低。把数据传输的可靠性保证放在运输层，会更加合适。



##### 停止等待协议

要实现可靠传输，最简便的方法就是：我发送一个数据包给你，然后你跟我回复收到，我继续发送下一个数据包。

![20220506-7](../../Image/2022/05/220506-7.png)



这种“一来一去”的方法来保证传输可靠就是**停止等待协议**（stop-and-wait）。TCP首部有一个ack字段，当设置为1时，表示这个报文是一个确认收到报文。



##### 超时重传

然后再来考虑一种情况：丢包。网络环境不可靠，导致每一次发送的数据包可能会丢失，如果机器A发送了数据包丢失了，那么机器B永远接收不到数据，机器A永远在等待。解决这个问题的方法是：**超时重传** 。当机器A发出一个数据包时便开始计时，时间到还没收到确认回复，就可以认为是发生了丢包，便再次发送，也就是重传。

但重传会导致另一种问题：如果原先的数据包并没有丢失，只是在网络中待的时间比较久，这个时候机器B会受到两个数据包，那么机器B是如何辨别这两个数据包是属于同一份数据还是不同的数据？这就需要前面讲过的方法：**给数据字节进行编号**。这样接收方就可以根据数据的字节编号，得出这些数据是接下来的数据，还是重传的数据。

在TCP首部有两个字段：序号和确认号，他们表示发送方数据第一个字节的编号，和接收方期待的下一份数据的第一个字节的编号。前面讲到TCP是面向字节流，但是他并不是一个字节一个字节地发送，而是一次截取一整段。截取的长度受多种因素影响，如缓存区的数据大小、数据链路层限制的帧大小等。



##### 连续ARQ协议

停止等待协议已经可以满足可靠传输了，但有一个致命缺点：**效率太低**。发送方发送一个数据包之后便进入等待，这个期间并没有干任何事，浪费了资源。解决的方法是：**连续发送数据包**。模型如下：

![20220506-8](../../Image/2022/05/220506-8.png)



和停止等待最大的不同就是，他会源源不断地发送，接收方源源不断收到数据之后，逐一进行确认回复。这样便极大地提高了效率。但同样，带来了一些额外的问题。



###### 滑动窗口

发送是否可以无限发送直到把缓冲区所有数据发送完？不可以。因为需要考虑接收方缓冲区以及读取数据的能力。如果发送太快导致接收方无法接受，那么只是会频繁进行重传，浪费了网络资源。所以发送方发送数据的范围，需要考虑到接收方缓冲区的情况。这就是TCP的**流量控制** 。解决方法是：**滑动窗口** 。基本模型如下：

![20220506-9](../../Image/2022/05/220506-9.png)



- 发送方需要根据接收方的缓冲区大小，设置自己的可发送窗口大小，处于窗口内的数据表示可发送，之外的数据不可发送。
- 当窗口内的数据接收到确认回复时，整个窗口会往前移动，直到发送完成所有的数据

在TCP的首部有一个窗口大小字段，他表示接收方的剩余缓冲区大小，让发送方可以调整自己的发送窗口大小。通过滑动窗口，就可以实现TCP的流量控制，不至于发送太快，导致太多的数据丢失。



**发送方滑动窗口**

- Not Sent，Recipient Not Ready to Receive。数据属于**发送端****未发送**，同时**接收端也未准备接收的**。
- Not Sent，Recipient Ready to Receive。数据是**发送端未发送**，**但已经告知接收方的，这部分其实已经在****窗口中（发送端缓存）**了，等待发送。
- Send But Not Yet Acknowledged。数据称为**发送但没有被确认**，数据被发送出去，没有收到接收端的 ACK，认为并没有完成发送，这个属于**窗口内的数据**。
- Sent and Acknowledged。数据表示**已经发送成功并已经被确认的数据**，这些数据已经离开窗口了。



**接收方滑动窗口**

- Not Received。有空位，还没有被接收的数据。
- eceived Not ACK。已经接收并，但是还没有回复 ACK，这些包可能输属于 Delay ACK 的范畴了。
- Received and ACK Not Send to Process。数据属于接收了数据但是还没有被上层的应用程序接收，也是被缓存在窗口内。
- Received and ACK Send to Process。离开了窗口缓存。



![图片](../../Image/2022/10/221009-1.png)



**滑动窗口的正式定义如下：**

1. Left edge和Right edge分别表示滑动窗口的左边界和右边界。
2. Usable Window：表示窗口的缓冲区。
3. Send Window ：发送窗口， 这部分值是有接收方在三次握手的时候进行设置的，同时在接收过程中也不断的通告可以发送的窗口大小，来进行适应。
4. Window Already Sent: 已经发送的数据，但是并没有收到 ACK。



滑动窗口指"发送方希望接收到的以ACK标志开头的数据字节数"。滑动窗口是跟ACK一起的，因此ACK标志必须置为1，同时指定窗口大小。滑动窗口大小通过16个bit来描述，所以变化范围0-65535（这个范围其实是可以缩放的）。



**工作原理**

TCP不是每个报文段都会返回TCP的，可能对**多个报文返回一个AC****K**。

假设依次发送3个报文（A，B，C）。如果B，C报文先到，那么先把A的位置预留出来，这个预留的地方称之为**空洞**（hole），只有A报文到达了，接收端才返回**一个ACK（不是3个）**进行确认**。**

最大段大小MSS （Max Segment Size），**数据被TCP分割成合适发送的数据块**，称为段(Segment)。这里说的段(Segment)不包括协议首部，**只包含数据**！

1. 有一组数据通过TCP传输，TCP先 将其分成若干段，假设有四个段seg1,seg2,seg3,seg4，依次发送出去，此时假设接收端接收到了 seg1 seg2 seg4；
2. 此时接收端的行为是回复一个 ACK 包说明已经接收到，并将 seg4 进行缓存（保证顺序，产生一个保存 seg3 的 hole）；
3. 发送端收到 ACK 之后，就会将对应的数据包变为已确认状态，这个时候窗口向右移动；
4. 假设接收端通告的 Window Size 仍然不变，此时窗口右移，产生一些新的空位，这些是接收端允许发送的范畴；
5. 对于丢失的 seg3，如果超过一定时间，TCP 就会重新传送（重传机制），重传成功会 seg3 seg4 一块被确认，不成功，seg4 也将被丢弃。



**TCP滑动窗口主要有以下作用：**

1. TCP在滑动窗口的基础上提供流量控制，防止较快主机致使较慢主机的缓冲区溢出，主要是根据网络情况**动态调整窗口大小**；

2. TCP将数据分段发送，确保发送的数据到达接收端也是正确的。利用的是“空洞”。TCP不会对每个数据包都返回ACK，而是累计返回的。



###### 累计确认

连续ARQ带来的第二个问题是：网络中充斥着和发送数据包一样数据量的确认回复报文，因为每一个发送数据包，必须得有一个确认回复。提高网络效率的方法是：**累积确认** 。接收方不需要逐个进行回复，而是累积到一定量的数据包之后，告诉发送方，在此数据包之前的数据全都收到。例如，收到 1234，接收方只需要告诉发送方我收到4了，那么发送方就知道1234都收到了。



###### GBN

连续ARQ第三个问题是：如何处理丢包情况。在停止等待协议中很简单，直接一个超时重传就解决了。但，连续ARQ中不太一样。例如：接收方收到了 123 567，六个字节，编号为4的字节丢失了。按照累积确认的思路，只能发送3的确认回复，567都必须丢掉，因为发送方会进行重传。这就是**GBN（go-back-n)** 思路。



###### 选择确认SACK

但是我们会发现，只需要重传4即可，这样不是很浪费资源，所以就有了：**选择确认SACK** 。在TCP报文的选项字段，可以设置已经收到的报文段，每一个报文段需要两个边界来进行确定。这样发送方，就可以根据这个选项字段只重传丢失的数据了。



##### 总结

- 通过连续ARQ协议与发送-确认回复模式来保证每一个数据包都到达接收方
- 通过给字节编号的方法，来标记每一个数据是属于重传还是新的数据
- 通过超时重传的方式，来解决数据包在网络中丢失的问题
- 通过滑动窗口来实现流量控制
- 通过累积确认+选择确认的方法来提高确认回复与重传的效率



#### 拥塞控制 

拥塞控制考虑的是另外一个问题：**避免网络过分拥挤导致丢包严重，网络效率降低 。**

这里的拥塞控制和前面的流量控制不是一个东西，流量控制是拥塞控制的手段：为了避免拥塞，必须对流量进行控制。拥塞控制目的是：限制每个主机的发送的数据量，避免网络拥塞效率下降。就像广州等地，限制车牌号出行是一个道理。不然大家都堵在路上，谁都别想走。

拥塞控制的解决方法是流量控制，流量控制的实现是滑动窗口，所以**拥塞控制最终也是通过限制发送方的滑动窗口大小来限制流量** 。当然，拥塞控制的手段不只是流量控制，导致拥塞的因素有：路由器缓存、带宽、处理器处理速度等等。提升硬件能力（把4车道改成8车道）是其中一个方法，但毕竟硬件提升是有瓶颈的，没办法不断提升，还是需要从tcp本身来增加算法，解决拥塞。



拥塞控制的重点有4个：**慢开始、快恢复、快重传、拥塞避免**。这里依旧献祭出大学老师的ppt图片：

![20220506-10](../../Image/2022/05/220506-10.png)

Y轴表示的是发送方窗口大小，X轴表示的是发送的轮次（不是字节编号）。

- 最开始的时候，会把窗口设置一个较小的值，然后每轮变为原来的两倍。这是慢开始。
- 当窗口值到达ssthresh值，这个值是需要通过实时网络情况设置的一个窗口限制值，开始进入拥塞避免，每轮把窗口值提升1，慢慢试探网络的底线。
- 如果发生了数据超时，表示极可能发生了拥塞，然后回到慢开始，重复上面的步骤。
- 如果收到三个相同的确认回复，表示现在网络的情况不太好，把ssthresh的值设置为原来的一半，继续拥塞避免。这部分称为快恢复。
- 如果收到丢包信息，应该尽快把丢失的包重传一次，这是快重传。
- 当然，窗口的最终上限是不能无限上涨的，他不能超过接收方的缓存区大小。

通过这个算法，就可以在很大程度上，避免网络拥挤。

除此之外，还可以让路由器在缓存即将满的时候，告知发送方我快满了，而不是等到出现了超时再进行处理，这是**主动队列管理AQM**。此外还有很多方法，但是上面的算法是重点。



#### 面向连接

TCP是面向连接的，那连接是什么？**这里的连接并不是实实在在的连接，而是通信双方彼此之间的一个记录** 。TCP是一个全双工通信，也就是可以互相发送数据，所以双方都需要记录对方的信息。根据前面的可靠传输原理，TCP通信双方需要为对方准备一个接收缓冲区可以接收对方的数据、记住对方的socket知道怎么发送数据、记住对方的缓冲区来调整自己的窗口大小等等，这些记录，就是一个连接。



在运输层小节中讲到，运输层双方通信的地址是采用socket来定义的，TCP也不例外。TCP的每一个连接只能有两个对象，也就是两个socket，而不能有三个。所以socket的定义需要源IP、源端口号、目标IP、目标端口号四个关键因素，才不会发生混乱。



>假如TCP和UDP一样只采用目标IP+目标端口号来定义socket，那么就会出现多个发送方同时发送到同一个目标socket的情况。这个时候TCP无法区分这些数据是否来自不同的发送方，就会导致出现错误。



##### 建立连接

建立连接的目的就是交换彼此的信息，然后记住对方的信息。所以双方都需要发送彼此的信息给对方。

但前面的可靠传输原理告诉我们，数据在网络中传输是不可靠的，需要对方给予我们一个确认回复，才可以保证消息正确到达。如下图：

机器B的确认收到和机器B信息可以进行合并，减少次数；而且发送机器B给机器A本身就代表了机器B已经收到了消息，所以最后的示例图是：

![20220506-11](../../Image/2022/05/220506-11.png)



步骤如下：

1. 机器A发送syn包向机器B请求建立TCP连接，并附加上自身的接收缓冲区信息等，机器A进入SYN_SEND状态，表示请求已经发送正在等待回复；
2. 机器B收到请求之后，根据机器A的信息记录下来，并创建自身的接收缓存区，向机器A发送syn+ack的合成包，同时自身进入SYN_RECV状态，表示已经准备好了，等待机器A 的回复就可以向A发送数据；
3. 机器A收到回复之后记录机器B 的信息，发送ack信息，自身进入ESTABLISHED状态，表示已经完全准备好了，可以进行发送和接收；
4. 机器B收到ACK数据之后，进入ESTABLISHED状态。

三次消息的发送，称为三次握手。



**为什么要三次而不是两次握手？**

两次握手的情况下，客户端发送syn包时如果没有到达服务器，会重发syn包，第二次发送syn包后建立连接。此时第一次syn包到达服务器时，服务器会认为是新的连接并在两次握手后进入等待数据状态，而客户端会认为是一个连接，导致状态不一致。

三次握手的情况下，第一次syn包没有到达时，服务端收不到最后的ack包，就不会建立连接。



##### 断开连接

断开连接和三次握手类似，直接上图：

![20220506-12](../../Image/2022/05/220506-12.png)



1. 机器A发送完数据之后，向机器B请求断开连接，自身进入FIN_WAIT_1状态，表示数据发送完成且已经发送FIN包（FIN标志位为1）；

2. 机器B收到FIN包之后，回复ack包表示已经收到，但此时机器B可能还有数据没发送完成，自身进入CLOSE_WAIT状态，表示对方已发送完成且请求关闭连接，自身发送完成之后可以关闭连接；

3. 机器B数据发送完成之后，发送FIN包给机器B ，自身进入LAST_ACK状态，表示等待一个ACK包即可关闭连接；

4. 机器A收到FIN包之后，知道机器B也发送完成了，回复一个ACK包，并进入TIME_WAIT状态

*TIME_WAIT状态比较特殊。当机器A收到机器B的FIN包时，理想状态下，确实是可以直接关闭连接了；但是：*



1. 我们知道网络是不稳定的，可能机器B 发送了一些数据还没到达（比FIN包慢）；
2. 同时回复的ACK包可能丢失了，机器B会重传FIN包；



*如果此时机器A马上关闭连接，会导致数据不完整、机器B无法释放连接等问题。所以此时机器A需要等待2个报文生存最大时长，确保网络中没有任何遗留报文了，再关闭连接*

5. 最后，机器A等待两个报文存活最大时长之后，机器B 接收到ACK报文之后，均关闭连接，进入CLASED状态

双方之间4次互相发送报文来断开连接的过程，就是四次挥手。

现在，对于为什么握手是三次挥手是四次、一定要三次/四次吗、为什么要停留2msl再关闭连接等等这些问题，就都解决了。



###### 等待超时时间

为了保证对方已经收到ACK包，因为假设客户端发送完最后一包ACK包后释放了连接，一旦ACK包在网络中丢失，服务端将一直停留在最后确认状态。如果等待一段时间，这时服务端会因为没有收到ack包重发FIN包，客户端会响应 这个FIN包进行重发ack包，并刷新超时时间。这个机制跟第三次握手一样，也是为了保证在不可靠的网络链路中进行可靠的连接断开确认。



#### 总结

①数据分片：在发送端对用户数据进行分片，在接收端进行重组，由TCP确定分片的大小并控制分片和重组；

②到达确认：接收端接收到分片数据时，根据分片数据序号向发送端发送一个确认；

③超时重发：发送方在发送分片时启动超时定时器，如果在定时器超时之后没有收到相应的确认，重发分片；

④滑动窗口：TCP连接每一方的接收缓冲空间大小都固定，接收端只允许另一端发送接收端缓冲区所能接纳的数据，TCP在滑动窗口的基础上提供流量控制，防止较快主机致使较慢主机的缓冲区溢出；

⑤失序处理：作为IP数据报来传输的TCP分片到达时可能会失序，TCP将对收到的数据进行重新排序，将收到的数据以正确的顺序交给应用层；

⑥重复处理：作为IP数据报来传输的TCP分片会发生重复，TCP的接收端必须丢弃重复的数据；

⑦数据校验：TCP将保持它首部和数据的检验和，这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到分片的检验和有差错，TCP将丢弃这个分片，并不确认收到此报文段导致对端超时并重发。



### UDP协议

UDP（User Data Protocol，用户数据报协议）是与TCP相对应的协议。它是面向非连接的协议，它不与对方建立连接，而是直接就把数据包发送过去！ 
  UDP适用于一次只传送少量数据、对可靠性要求不高的应用环境。比如，我们经常使用“ping”命令来测试两台主机之间TCP/IP通信是否正常，其实“ping”命令的原理就是向对方主机发送UDP数据包，然后对方主机确认收到数据包，如果数据包是否到达的消息及时反馈回来，那么网络就是通的。例如，在默认状态下，一次“ping”操作发送4个数据包（如图2所示）。大家可以看到，发送的数据包数量是4包，收到的也是4包（因为对方主机收到后会发回一个确认收到的数据包）。这充分说明了UDP协议是面向非连接的协议，没有建立连接的过程。正因为UDP协议没有连接的过程，所以它的通信效果高；但也正因为如此，它的可靠性不如TCP协议高。QQ就使用UDP发消息，因此有时会出现收不到消息的情况。 



#### 功能特点

运输层协议除了TCP，还有大名鼎鼎的UDP。如果说TCP凭借他完善稳定的功能独树一帜，那UDP就是精简主义乱拳打死老师傅。



UDP只实现了运输层最少的功能：进程间通信。对于应用层传下来的数据，UDP只是附加一个首部就直接交给网络层了。UDP的头部非常简单，只有三部分：



- 源端口、目标端口：端口号用来区分主机的不同进程
- 校验码：用于校验数据包在传输的过程中没有出现错误，例如某个1变成了0
- 长度：报文的长度



所以UDP的功能也只有两个：校验数据报是否发生错误、区分不同的进程通信。

但，TCP的功能虽然多，但同时也是要付出相对应的代价。例如面向连接的特性，在建立和断开连接的时候会有开销；拥塞控制的特性，会限制传输的上限等等。



#### 优点和缺点

##### UDP的缺点

- 无法保证消息完整、正确到达，UDP是一个不可靠的传输协议；
- 缺少拥塞控制容易互相竞争资源导致网络系统瘫痪



##### UDP的优点

- 效率更快；不需要建立连接以及拥塞控制

- 连接更多的客户；没有连接状态，不需要为每个客户创建缓存等

- 分组首部字节少，开销小；TCP首部固定首部是20字节，而UDP只有8字节；更小的首部意味着更大比例的数据部分

- 在一些需要高效率允许可限度误差的场景下可以使用。如直播场景，并不需要保证每个数据包都完整到达，允许一定的丢包率，这个时候TCP的可靠特性反而成为了累赘；精简的UDP更高的效率是更加适合的选择

- 可以进行广播；UDP并不是面向连接的，所以可以同时对多个进程进行发送报文

    

#### 适用场景

UDP适用于对传输模型需要应用层高度自定义、允许出现丢包、需要高效率的场景、需要广播；例如

- 视屏直播
- DNS
- RIP路由选择协议



### TCP和UDP比较

|            | TCP                            | UDP                            |
| ---------- | ------------------------------ | ------------------------------ |
| 是否连接   | 面向连接                       | 面向非连接                     |
| 传输可靠性 | 可靠                           | 不可靠                         |
| 应用场合   | 传输大量数据                   | 传输少量数据                   |
| 速度       | 慢                             | 快                             |
| 使用场景   | 传输文件，发送邮件，浏览网页等 | 域名查询，语音通话，视频直播等 |



## 网络传输

### 路由转换

| 主机  | 路由    | 路由    | 路由    | 路由    | 主机    |
| ----- | ------- | ------- | ------- | ------- | ------- |
|       | > 路由1 | > 路由3 |         | > 路由7 |         |
| 主机A |         | > 路由4 | > 路由6 |         | > 主机B |
|       | > 路由2 | > 路由5 |         | > 路由8 |         |



- 正常情况下，主机A的数据包可以由 1-3-6-7路径进行传送
- 如果路由3坏掉了，那么可以从 1-4-6-7进行传送
- 如果4也坏掉了，那么只能从2-5-6-7传送
- 如果5坏掉了，那么就中断线路了



使用路由转发可以提高网络的容错率，本质原因依旧是网络是不稳定的，即使坏掉几个路由器，网络依旧畅通。但是如果坏掉路由器6那就直接导致主机A和主机B无法通信，所以要避免这种核心路由器的存在。

使用路由转发还可以分流，如果一条线路太拥堵，可以从别的路线进行传输，提高效率。



### 恶意攻击

TCP的面向连接特点可能会被恶意的人利用，对服务器进行攻击。



前面我们知道，当我们向一个主机发送syn包请求创建连接时，服务器会为我们创建缓冲区等，然后向我们返回syn+ack报文；如果我们伪造IP和端口，向一个服务器进行海量的请求，会使得服务器创建了大量的创建一半的TCP连接，使得其无法正常响应用户的请求，导致服务器瘫痪。



解决的方法可以有限制IP的创建连接数、让创建一半的tcp连接在更短的时间内自行关闭、延缓接收缓冲区内存的分配等等。



### 长连接

我们向服务器的每一次请求都需要创建一个TCP连接，服务器返回数据之后就会关闭连接；如果在短时间内有大量的请求，那么频繁创建TCP连接关闭TCP连接是一个很浪费资源的行为。所以我们可以让TCP连接不要关闭，在这个期间进行请求，提高效率。

需要注意长连接维持时间、创建条件等，避免被恶意利用创建大量的长连接，消耗殆尽服务器的资源。



# RPC

**RPC**（**R**emote **P**rocedure **C**all），又叫做**远程过程调用**。它本身并不是一个具体的协议，而是一种**调用方式**。

RPC协议，比较有名的有`gRPC`，`thrift`。虽然大部分RPC协议底层使用TCP，但实际上**它们不一定非得使用TCP，改用UDP或者HTTP，其实也可以做到类似的功能。**



# HTTP
## HTTP特性

**HTTP**协议（**H**yper **T**ext **T**ransfer **P**rotocol），又叫做**超文本传输协议**。

标准的http协议指的是不包括cookies, session，application的http协议，他们都不属于标准协议。



### 有状态和无状态
标准的http协议是无状态的，无连接的。 

TCP一直有状态，HTTP一直无状态，但是应用为了有状态，就给HTTP加了cookie和session机制，让使用http的应用也能有状态，但http还是无状态。



### 有链接和无连接
无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。

开始TCP是有连接，后来TCP无连接，再后来也就是现在TCP是Keep-Alive，有点像有连接。



## 状态码

### 200

### 300

### 400

### 500



## HTTP和RPC的区别

现在电脑上装的各种**联网**软件，比如xx管家，xx卫士，它们都作为**客户端（client）**需要跟**服务端（server）**建立连接收发消息，此时都会用到应用层协议，在这种**client/server (c/s)**架构下，它们可以使用自家造的RPC协议，因为它只管连自己公司的服务器就ok了。

但有个软件不同，**浏览器（browser）**，不管是chrome还是IE，它们不仅要能访问自家公司的**服务器（server）**，还需要访问其他公司的网站服务器，因此它们需要有个统一的标准，不然大家没法交流。于是，HTTP就是那个时代用于统一 **browser/server (b/s)** 的协议。

**HTTP主要用于b/s架构，而RPC更多用于c/s架构。但现在其实已经没分那么清了，b/s和c/s在慢慢融合。**



### 服务发现

首先要向某个服务器发起请求，你得先建立连接，而建立连接的前提是，你得知道**IP地址和端口**。这个找到服务对应的IP端口的过程，其实就是**服务发现**。

在**HTTP**中，你知道服务的域名，就可以通过**DNS服务**去解析得到它背后的IP地址，默认80端口。

而**RPC**的话，就有些区别，一般会有专门的**中间服务**去保存服务名和IP信息，比如**consul或者etcd，甚至是redis**。想要访问某个服务，就去这些中间服务去获得IP和端口信息。由于dns也是服务发现的一种，所以也有基于dns去做服务发现的组件，比如**CoreDNS**。



### 底层连接形式

以主流的**HTTP1.1**协议为例，其默认在建立底层TCP连接之后会一直保持这个连接（**keep alive**），之后的请求和响应都会复用这条连接。

而**RPC**协议，也跟HTTP类似，也是通过建立TCP长链接进行数据交互，但不同的地方在于，RPC协议一般还会再建个**连接池**，在请求量大的时候，建立多条连接放在池内，要发数据的时候就从池里取一条连接出来，**用完放回去，下次再复用**，可以说非常环保。

![图片](../../Image/2022/09/220926-8.jpg)



**由于连接池有利于提升网络请求性能，所以不少编程语言的网络库里都会给HTTP加个连接池**，比如**go**就是这么干的。



### 传输的内容

基于TCP传输的消息，说到底，无非都是**消息头header和消息体body。**

**header**是用于标记一些特殊信息，其中最重要的是**消息体长度**。

**body**则是放我们真正需要传输的内容，而这些内容只能是二进制01串，毕竟计算机只认识这玩意。所以TCP传字符串和数字都问题不大，因为字符串可以转成编码再变成01串，而数字本身也能直接转为二进制。但结构体呢，我们得想个办法将它也转为二进制01串，这样的方案现在也有很多现成的，比如**json，protobuf。**

这个将结构体转为二进制数组的过程就叫**序列化**，反过来将二进制数组复原成结构体的过程叫**反序列化**。

![图片](../../Image/2022/09/220926-10.jpg)



对于主流的HTTP1.1，虽然它现在叫超文本协议，支持音频视频，但HTTP设计初是用于做网页文本展示的，所以它传的内容以字符串为主。header和body都是如此。在body这块，它使用json来序列化结构体数据。

![图片](../../Image/2022/09/220926-9.jpg)

对于主流的HTTP1.1，虽然它现在叫**超文本**协议，支持音频视频，但HTTP设计初是用于做网页**文本**展示的，所以它传的内容以字符串为主。header和body都是如此。在body这块，它使用**json**来**序列化**结构体数据。

![图片](../../Image/2022/09/220926-11.jpg)



而RPC，因为它定制化程度更高，可以采用体积更小的protobuf或其他序列化协议去保存结构体数据，同时也不需要像HTTP那样考虑各种浏览器行为，比如302重定向跳转啥的。**因此性能也会更好一些，这也是在公司内部微服务中抛弃HTTP，选择使用RPC的最主要原因。**

![图片](../../Image/2022/09/220926-12.jpg)



### 总结

上面说的HTTP，其实**特指的是现在主流使用的HTTP1.1**，`HTTP2`在前者的基础上做了很多改进，所以**性能可能比很多RPC协议还要好**，甚至连`gRPC`底层都直接用的`HTTP2`。

但是由于HTTP2是2015年出来的。那时候很多公司内部的RPC协议都已经跑了好些年了，基于历史原因，一般也没必要去换了。



## 长短链接

长链接：顾名思义，就是网页的完整URL地址，点击即可跳转至网页，进行内容浏览。

短链接：就是将长链接进行处理后转换成长度较小的URL地址，如 https://sourl.cn/upNbxj  则是长链接 https://blog.csdn.net/qq_39486758/article/details/126602389 处理之后的结果。

短链接相较于长链接，会更简短，便于一些第三方平台的字符长度限制等问题处理。



### 长短链接原理

在网站输入**短链接**后，**DNS**会解析链接的ip地址（即**短链接服务器**），然后DNS转发请求（**HTTP GET**）至短链接服务器，通过短链接码换取对应的**完整URL地址**，最后短链接服务器通过请求（**HTTP 301**）重定向到完整URL地址，至此完成解析。可以参考时序图：

![图片](../../Image/2022/10/221013-3.png)

短链接跳转长链接可以采用301（永久重定向），也可以采用302（临时重定向），区别就是对资源的管理，301会将旧资源永久移除，替换为重定向的新资源；而302还是会保留旧资源，只是重定向到新资源，并不会发生替换，也不会保存新资源。



# HTTPS
## HTTPS特性
HTTP 由于是明文传输，所谓的明文，就是说客户端与服务端通信的信息都是肉眼可见的，随意使用一个抓包工具都可以截获通信的内容。

所以安全上存在以下三个风险：

- 窃听风险，比如通信链路上可以获取通信内容，用户号容易没。
- 篡改风险，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。
- 冒充风险，比如冒充淘宝网站，用户钱容易没。

HTTPS 在 HTTP 与 TCP 层之间加入了 TLS 协议，来解决上述的风险。

TLS 协议包括如下三个部分：
信息加密：HTTP 交互信息是被加密的，第三方就无法被窃取；
校验机制：校验信息传输过程中是否有被第三方篡改过，如果被篡改过，则会有警告提示；
身份证书：证明淘宝是真的淘宝网；

![1](../../Image/2021/03/210319.webp)

## HTTPS流程

### TLS

TLS 握手的目的就是为了协商出会话密钥，也就是对称加密密钥。



## HTTPS优化

由裸数据传输的 HTTP 协议转成加密数据传输的 HTTPS 协议，给应用数据套了个保护伞，提高安全性的同时也带来了性能消耗。

因为 HTTPS 相比 HTTP 协议多一个 TLS 协议握手过程，**目的是为了通过非对称加密握手协商或者交换出对称加密密钥**，这个过程最长可以花费掉 2 RTT，接着后续传输的应用数据都得使用对称加密密钥来加密/解密。



产生性能消耗的两个环节：

- 第一个环节， TLS 协议握手过程；
- 第二个环节，握手后的对称加密报文传输。



第一个环节，TLS 协议握手过程不仅增加了网络延时（最长可以花费掉 2 RTT），而且握手过程中的一些步骤也会产生性能损耗，比如：

- 对于 ECDHE 密钥协商算法，握手过程中会客户端和服务端都需要临时生成椭圆曲线公私钥；
- 客户端验证证书时，会访问 CA 获取 CRL 或者 OCSP，目的是验证服务器的证书是否有被吊销；
- 双方计算 Pre-Master，也就是对称加密密钥；

第二环节，现在主流的对称加密算法 AES、ChaCha20 性能都是不错的，而且一些 CPU 厂商还针对它们做了硬件级别的优化，因此这个环节的性能消耗可以说非常地小。



![221002-1](../../Image/2022/221002-1.png)



### 硬件优化

**HTTPS 协议是计算密集型，而不是 I/O 密集型**，所以应该提高提高CPU性能，而不是网卡、硬盘等。

选择**支持 AES-NI 特性的 CPU**，这种 CPU 能在指令级别优化 AES 算法，加速了数据的加解密传输过程。Linux系统查看CPU是否支持AES-NI指令集命令如下：

```bash
$sort -u /proc/crypto | grep module | grep aes module : aesni_intel
```



或者选择 ChaCha20 对称加密算法，因为 ChaCha20 算法的运算指令相比 AES 算法会对 CPU 更友好。



### 软件优化

软件升级就是将正在使用的软件升级到最新版本，因为最新版本不仅提供了最新的特性，也优化了以前软件的问题或性能。比如：

- 将 Linux 内核从 2.x 升级到 4.x；
- 将 OpenSSL 从 1.0.1 升级到 1.1.1；



### 会话复用

把首次 TLS 握手协商的对称加密密钥缓存起来，待下次需要建立 HTTPS 连接时，直接「复用」这个密钥来减少 TLS 握手的性能损耗了吗。这种方式就是**会话复用**（*TLS session resumption*），会话复用分两种：

- 第一种叫 Session ID；
- 第二种叫 Session Ticket；



#### Session ID

Session ID 的工作原理是，**客户端和服务器首次  TLS 握手连接后，双方会在内存缓存会话密钥，并用唯一的 Session ID 来标识**，Session ID 和会话密钥相当于 key-value 的关系。

当客户端再次连接时，hello 消息里会带上 Session ID，服务器收到后就会从内存找，如果找到就直接用该会话密钥恢复会话状态，跳过其余的过程，只用一个消息往返就可以建立安全通信。当然为了安全性，内存中的会话密钥会定期失效。

![图片](../../Image/2022/10/221002-6.png)

但是它有两个缺点：

- 服务器必须保持每一个客户端的会话密钥，随着客户端的增多，**服务器的内存压力也会越大**。
- 现在网站服务一般是由多台服务器通过负载均衡提供服务的，**客户端再次连接不一定会命中上次访问过的服务器**，于是还要走完整的 TLS 握手过程；



#### Session Ticket

为了解决 Session ID 的问题，就出现了 Session Ticket，**服务器不再缓存每个客户端的会话密钥，而是把缓存的工作交给了客户端**，类似于 HTTP 的 Cookie。

客户端与服务器首次建立连接时，服务器会加密「会话密钥」作为 Ticket 发给客户端，交给客户端缓存该 Ticket。

客户端再次连接服务器时，客户端会发送 Ticket，服务器解密后就可以获取上一次的会话密钥，然后验证有效期，如果没问题，就可以恢复会话了，开始加密通信。

![图片](../../Image/2022/10/221002-7.png)

对于集群服务器的话，**要确保每台服务器加密 「会话密钥」的密钥是一致的**，这样客户端携带 Ticket 访问任意一台服务器时，都能恢复会话。

Session ID 和 Session Ticket **都不具备前向安全性**，因为一旦加密「会话密钥」的密钥被破解或者服务器泄漏「会话密钥」，前面劫持的通信密文都会被破解。同时应对**重放攻击**也很困难。



#### Pre-shared Key

Session ID 和 Session Ticket 方式都需要在 1 RTT 才能恢复会话。对于重连 TLS1.3 只需要 **0 RTT**，原理和 Ticket 类似，只不过在重连时，客户端会把 Ticket 和 HTTP 请求一同发送给服务端，这种方式叫 **Pre-shared Key**。

![图片](../../Image/2022/10/221002-8.png)

Pre-shared Key 也有**重放攻击**的危险。

![图片](../../Image/2022/10/221002-9.png)

假设中间人通过某种方式，截获了客户端使用会话重用技术的 POST 请求，通常 POST 请求是会改变数据库的数据，然后中间人就可以把截获的这个报文发送给服务器，服务器收到后，也认为是合法的，于是就恢复会话，致使数据库的数据又被更改，但是此时用户是不知情的。

所以，应对重放攻击可以给会话密钥设定一个合理的过期时间，以及只针对安全的 HTTP 请求如 GET/HEAD 使用会话重用。



### 协议优化

#### 密钥交换算法优化

TLS 1.2 版本如果使用的是 RSA 密钥交换算法，那么需要 4 次握手，也就是要花费 2 RTT，才可以进行应用数据的传输，而且 RSA 密钥交换算法不具备前向安全性。**RSA 密钥交换算法的 TLS 握手过程，不仅慢，而且安全性也不高**。

对于对称加密算法方面，如果对安全性不是特别高的要求，可以**选用 AES_128_GCM**，它比 AES_256_GCM 快一些，因为密钥的长度短一些。



尽量**选用 ECDHE 密钥交换**算法替换 RSA 算法，因为该算法由于支持False Start，它是“抢跑”的意思，客户端可以在 TLS 协议的第 3 次握手后，第 4 次握手前，发送加密的应用数据，以此将 **TLS 握手的消息往返由 2 RTT 减少到 1 RTT，而且安全性也高，具备前向安全性**。

ECDHE 算法是基于椭圆曲线实现的，不同的椭圆曲线性能也不同，应该尽量**选择 x25519 曲线**，该曲线是目前最快的椭圆曲线。



##### Nginx配置

在 Nginx 上，可以使用 ssl_ecdh_curve 指令配置想使用的椭圆曲线，把优先使用的放在前面：

```bash
ssl_ecdh_curve X255119:secp384r1;
```



在 Nginx 上，使用 ssl_ciphers 指令配置想使用的非对称加密算法和对称加密算法，也就是密钥套件，而且把性能最快最安全的算法放在最前面：

```
ssl_ciphers 'EECDH+ECDSA+AES128+SHA:RSA+AES128+SHA'
```



#### TLS 升级

把 TLS 1.2 升级成 TLS 1.3，TLS 1.3 大幅度简化了握手的步骤，**完成 TLS 握手只要 1 RTT**，而且安全性更高。

在 TLS 1.2 的握手中，一般是需要 4 次握手，先要通过 Client Hello （第 1 次握手）和 Server Hello（第 2 次握手） 消息协商出后续使用的加密算法，再互相交换公钥（第 3 和 第 4 次握手），然后计算出最终的会话密钥，下图的左边部分就是 TLS 1.2 的握手过程：

![图片](../../Image/2022/10/221002-2.png)



右边部分就是 TLS 1.3 的握手过程，可以发现 **TLS 1.3 把 Hello 和公钥交换这两个消息合并成了一个消息，于是这样就减少到只需 1 RTT 就能完成 TLS 握手**。

客户端在  Client Hello 消息里带上了支持的椭圆曲线，以及这些椭圆曲线对应的公钥。服务端收到后，选定一个椭圆曲线等参数，然后返回消息时，带上服务端这边的公钥。经过这 1 个 RTT，双方手上已经有生成会话密钥的材料了，于是客户端计算出会话密钥，就可以进行应用数据的加密传输了。



而且，TLS1.3 对密码套件进行“减肥”了，**对于密钥交换算法，废除了不支持前向安全性的  RSA 和 DH 算法，只支持 ECDHE 算法**。

对于对称加密和签名算法，只支持目前最安全的几个密码套件，比如 openssl 中仅支持下面 5 种密码套件：

- TLS_AES_256_GCM_SHA384
- TLS_CHACHA20_POLY1305_SHA256
- TLS_AES_128_GCM_SHA256
- TLS_AES_128_CCM_8_SHA256
- TLS_AES_128_CCM_SHA256

之所以 TLS1.3  仅支持这么少的密码套件，是因为 TLS1.2 由于支持各种古老且不安全的密码套件，中间人可以利用降级攻击，伪造客户端的 Client Hello 消息，替换客户端支持的密码套件为一些不安全的密码套件，使得服务器被迫使用这个密码套件进行 HTTPS 连接，从而破解密文。



### 证书优化

为了验证的服务器的身份，服务器会在 TLS 握手过程中，把自己的证书发给客户端，以此证明自己身份是可信的。

对于证书的优化，可以有两个方向：

- 一个是**证书传输**，
- 一个是**证书验证**；



#### 证书传输

要让证书更便于传输，那必然是减少证书的大小，这样可以节约带宽，也能减少客户端的运算量。所以，**对于服务器的证书应该选择椭圆曲线（ECDSA）证书，而不是 RSA 证书，因为在相同安全强度下， ECC 密钥长度比 RSA 短的多**。



#### 证书验证

客户端在验证证书时，是个复杂的过程，会走证书链逐级验证，验证的过程不仅需要「用 CA 公钥解密证书」以及「用签名算法验证证书的完整性」，而且为了知道证书是否被 CA 吊销，客户端有时还会再去访问 CA， 下载 CRL 或者 OCSP 数据，以此确认证书的有效性。

这个访问过程是 HTTP 访问，因此又会产生一系列网络通信的开销，如 DNS 查询、建立连接、收发数据等。

服务器应该开启 **OCSP Stapling** 功能，由服务器预先获得 OCSP 的响应，并把响应结果缓存起来，这样 TLS 握手的时候就不用再访问 CA 服务器，减少了网络通信的开销，提高了证书验证的效率。



##### CRL

CRL 称为证书吊销列表（*Certificate Revocation List*），这个列表是由 CA 定期更新，列表内容都是被撤销信任的证书序号，如果服务器的证书在此列表，就认为证书已经失效，不在的话，则认为证书是有效的。

![图片](../../Image/2022/10/221002-3.png)

但是 CRL 存在两个问题：

- 由于 CRL 列表是由 CA 维护的，定期更新，如果一个证书刚被吊销后，客户端在更新 CRL 之前还是会信任这个证书，**实时性较差**；
- **随着吊销证书的增多，列表会越来越大，下载的速度就会越慢**，下载完客户端还得遍历这么大的列表，那么就会导致客户端在校验证书这一环节的延时很大，进而拖慢了 HTTPS 连接。



##### OCSP

因此，现在基本都是使用 OCSP ，名为在线证书状态协议（*Online Certificate Status Protocol*）来查询证书的有效性，它的工作方式是**向 CA 发送查询请求，让 CA 返回证书的有效状态**。

![图片](../../Image/2022/10/221002-4.png)

不必像 CRL 方式客户端需要下载大大的列表，还要从列表查询，同时因为可以实时查询每一张证书的有效性，解决了 CRL 的实时性问题。

OCSP 需要向  CA 查询，因此也是要发生网络请求，而且还得看  CA 服务器的“脸色”，如果网络状态不好，或者 CA 服务器繁忙，也会导致客户端在校验证书这一环节的延时变大。



##### OCSP Stapling

于是为了解决这一个网络开销，就出现了 OCSP Stapling，其原理是：服务器向 CA 周期性地查询证书状态，获得一个带有时间戳和签名的响应结果并缓存它。

![图片](../../Image/2022/10/221002-5.png)

当有客户端发起连接请求时，服务器会把这个「响应结果」在 TLS 握手过程中发给客户端。由于有签名的存在，服务器无法篡改，因此客户端就能得知证书是否已被吊销了，这样客户端就不需要再去查询。

